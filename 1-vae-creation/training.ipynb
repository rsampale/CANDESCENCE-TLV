{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages and libraries used by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from dataloader import *\n",
    "from model import *\n",
    "from torchsummary import summary\n",
    "from loss_functions import *\n",
    "from utils import reconstruction_compare, create_loss_graph, dataframe_w_latentvecs, create_img_scatterplot, create_model_view_img_scatterplot\n",
    "from utils import create_annotated_scatterplot, get_latent_vectors, get_growth_medium, get_plate_id, zoom_img_scatterplot\n",
    "import hashlib\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the \"arguments\" dictionary, which contains many of the parameters, file paths, and simple variables used throughout the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "'DEVICE' : torch.device(\"cuda:5\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "\n",
    "'CANDESCENCE' : \"/home/data/refined/candescence\",\n",
    "'TLV' : \"/home/data/refined/candescence/tlv\",\n",
    "'VAES' : \"/home/data/refined/candescence/tlv/vaes\",\n",
    "'CUT_IMAGES' : os.path.join(\"/home/data/refined/candescence/tlv\", \"0.2-images_cut\", \"all_simple_statmatch\"),\n",
    "'SAVED_MODELS' : \"/home/data/refined/candescence/tlv/saved_torch_models\",\n",
    "'METADATA_DIR' : os.path.join(\"/home/data/refined/candescence/tlv\", \"data_files\", \"Calb_Master_10062022.tsv\"),\n",
    "'GRAPH_SAVES' : \"/home/data/refined/candescence/tlv/torch_model_graphs\",\n",
    "'MASTER_SAVES' : \"/home/data/refined/candescence/tlv/saved_MASTERs\",\n",
    "'LOSS_SAVES' : \"/home/data/refined/candescence/tlv/saved_loss_graphs\",\n",
    "\n",
    "'exp' : \"testing_simple_statmatch_2\",\n",
    "'dataset_seed' : 9954,\n",
    "'training_seed' : 9954,\n",
    "'image_type' : 'non-wash', # either \"wash\" or \"non-wash\"; can only specify this if taking images from all-final or a directory containing both wash and non-wash colonies\n",
    "'train_num' : 6400,\n",
    "'val_num' : 1600,\n",
    "'test_num' : 3200,\n",
    "\n",
    "'batch_size' : 64,\n",
    "'kernel_size' : 3,\n",
    "'latent_dim' : 6,\n",
    "'intermediate_dim' : 60,\n",
    "'epochs' : 20,\n",
    "'learning_rate' : 0.000125,\n",
    "'weight_decay' : 1.5e-5,\n",
    "'kl_weight' : 0.4,\n",
    "'MSE_weight' : 2.0,\n",
    "'size_bins' : 5,\n",
    "'intensity_bins' : 10,\n",
    "'OH_in_decoder' : False\n",
    "}\n",
    "\n",
    "print(f\"Device in use: {arguments['DEVICE']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wipe any previous instance of the log file so that only the current run is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('dataloading.log', 'w').close()\n",
    "# Note: all logging information is stored in dataloading.log, NOT just the logging information from the dataloader "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the dataloaders for the training, validation, and test datasets - to be used later during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = create_dataloader(arguments)\n",
    "\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a vae object using the class defined above, and also define our optimizer. Finally, we can send the model to the GPU/device used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(arguments['training_seed'])\n",
    "\n",
    "# Sample the third element in one of the dataloaders, which represents the length of the colony one-hot encodings\n",
    "# We pass this to our VAE class' __init__ so as to create a dynamic architecture that allows for changes to the one-hot encoding length\n",
    "sample_img, sample_OH = next(iter(train_dataloader))\n",
    "OH_len = sample_OH.shape[2]\n",
    "\n",
    "vae = VAE(arguments, OH_len, OH_in_decoder=arguments['OH_in_decoder'])\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=arguments['learning_rate'], weight_decay=arguments['weight_decay']) # potentially make weight decay into a modifiable hyperparameter above?\n",
    "\n",
    "# device = torch.device(\"cuda:7\")\n",
    "vae.to(arguments['DEVICE'])\n",
    "\n",
    "# summary(vae, (1,135,135))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define functions which handle the single epoch training and validation for the model, each returning the loss at the end of that epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(vae, device, dataloader, optimizer, arguments):\n",
    "\n",
    "    vae.train() # sets training mode for VAE's encoder and decoder\n",
    "    train_loss = 0.0\n",
    "    # imgs, OH_stuff = next(iter(dataloader))\n",
    "    # print(\"imgs: \", imgs)\n",
    "    # print(\"OH stuff: \", OH_stuff)\n",
    "    for x, one_hot in dataloader:\n",
    "        x = x.to(device)\n",
    "        one_hot = one_hot.to(device)\n",
    "        x_hat = vae(x, one_hot)\n",
    "        # print(f'input x: {x}')\n",
    "        # print(f'output x_hat: {x_hat}') ## x_hat gives tensors filled with nans\n",
    "        # print(f\"x min mean max: {x.min()}, {x.mean()}, {x.max()}\")\n",
    "\n",
    "        # Get loss with loss function\n",
    "        loss = get_MSE_kl_loss(x, x_hat, vae.encoder.sigma, vae.encoder.mu, arguments)\n",
    "\n",
    "        # Backward pass / weights modification\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print('\\t Single batch train loss: %f' % (loss.item()))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def validation_epoch(vae, device, dataloader, arguments):\n",
    "    vae.eval() # set evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # no need to track gradients\n",
    "        for x, one_hot in dataloader:\n",
    "            x = x.to(device)\n",
    "            one_hot = one_hot.to(device)\n",
    "\n",
    "            encoded_data = vae.encoder(x, one_hot) # pointless??\n",
    "            x_hat = vae(x, one_hot)\n",
    "            loss = get_MSE_kl_loss(x, x_hat, vae.encoder.sigma, vae.encoder.mu, arguments)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(dataloader.dataset)\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the training/validation loop, across the number of epochs specified above in the arguments/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(arguments['epochs']):\n",
    "   train_loss = training_epoch(vae,arguments['DEVICE'],train_dataloader,optimizer,arguments)\n",
    "   val_loss = validation_epoch(vae,arguments['DEVICE'],val_dataloader,arguments)\n",
    "\n",
    "   train_losses.append(train_loss)\n",
    "   val_losses.append(val_loss)\n",
    "\n",
    "   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, arguments['epochs'], train_loss, val_loss))\n",
    "   reconstruction_compare(arguments, test_dataloader.dataset, vae.encoder, vae.decoder, n=10)\n",
    "\n",
    "# Create/plot the losses over training period\n",
    "create_loss_graph(train_losses, val_losses, arguments)\n",
    "\n",
    "model_save_path = os.path.join(arguments['SAVED_MODELS'], f\"{arguments['exp']}.pth\")\n",
    "torch.save(vae.state_dict(), model_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will call upon utils functions to try and visualize / plot the latent space of the model, by using t-SNE and UMAP dimensionality reduction to reduce its dimensions to two. This can then easily be displayed on a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PUT MODEL IN EVALUATION/INFERENCE MODE: ######################\n",
    "vae.eval()\n",
    "###############                                         ######################\n",
    "\n",
    "test_dataset, test_indices = torch.load(os.path.join(arguments['VAES'], f'{arguments[\"dataset_seed\"]}_test'))\n",
    "full_dataset = test_dataset.dataset.dataset\n",
    "\n",
    "MASTER = dataframe_w_latentvecs(arguments, full_dataset, test_indices, vae)\n",
    "\n",
    "### Perform t-SNE dimensionality reduction on latent space ### \n",
    "\n",
    "features = MASTER.filter(regex=('^V\\d+'))\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "MASTER['tsne-2d-one'] = tsne_results[:,0]\n",
    "MASTER['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "### Perform UMAP dimensionality reduction on latent space ### \n",
    "\n",
    "umap = UMAP(n_components=2, random_state=42, min_dist=0.2, n_neighbors=15)\n",
    "umap_results = umap.fit_transform(features)\n",
    "\n",
    "MASTER['umap-2d-one'] = umap_results[:,0]\n",
    "MASTER['umap-2d-two'] = umap_results[:,1]\n",
    "\n",
    "\n",
    "# Save the MASTER dataframe for later use whenever needed\n",
    "file_name = arguments['exp'] + \"_MASTER.csv\"\n",
    "MASTER.to_csv(os.path.join(arguments['MASTER_SAVES'], file_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in the saved MASTER file for the experiment we want to analyze, and generate scatterplots representing the latent space with the original encoded images instead of points. Both t-SNE and UMAP can be used for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the image-overlay scatterplot\n",
    "\n",
    "exp_for_analysis = arguments['exp']\n",
    "file_name = exp_for_analysis + \"_MASTER.csv\"\n",
    "MASTER = pd.read_csv(os.path.join(arguments['MASTER_SAVES'], file_name))\n",
    "\n",
    "# SPECIFY DIMENSIONALITY REDUCTION TYPE AS UMAP = 'umap', t-SNE = 'tsne'\n",
    "create_img_scatterplot(MASTER, arguments, reduction_technique = 'tsne')\n",
    "create_img_scatterplot(MASTER, arguments, reduction_technique = 'umap')\n",
    "# create_model_view_img_scatterplot(MASTER, test_dataloader.dataset, arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next code block, we load in the saved MASTER file we want (by giving the experiment name), and can create annotated/colored scatterplots based off of any category of information that exists within the MASTER/METADATA (e.g. medium, geographical origin, strain, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST OF ANNOTATION CATEGORIES:\n",
    "# 'Plate'\n",
    "# 'medium'\n",
    "# 'day'\n",
    "# 'Broad.Niche'\n",
    "# 'Geography.General'\n",
    "# many more - see file stored in metadata_dir for all categories\n",
    "### END LIST\n",
    "# NOTE: Many boxes contain 'nan', 'NaN', or 'unknown' values - these are ideally filtered out or displayed as one color\n",
    "\n",
    "exp_for_analysis = arguments['exp']\n",
    "file_name = exp_for_analysis + \"_MASTER.csv\"\n",
    "MASTER = pd.read_csv(os.path.join(arguments['MASTER_SAVES'], file_name))\n",
    "\n",
    "# Select only a specific plate to be colored for visualization (if wanted):\n",
    "# conditions = (MASTER['Plate'] != 14) | (MASTER['day'] != 5) | (MASTER['medium'] != 'serum')\n",
    "# MASTER.loc[conditions, 'Plate'] = 0\n",
    "\n",
    "create_annotated_scatterplot(MASTER, 'medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call on the zoom_img_scatterplot() function to zoom in on a region of interest in either the UMAP or t-SNE scatterplots. The size of the zoomed colonies can be modified in the code for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates argument is in order: (xmin, xmax, ymin, ymax). \n",
    "# An additional final argument, \"annotate=\", can be passed to the function in order to pick an annotation style for the zoomed box. By default, it is \"None\".\n",
    "zoom_img_scatterplot(MASTER, arguments, 'tsne', (10, 42, -25, -10),annotate=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are a series of functions and function calls, whose primary purpose is to take a deeper look at and only plot certain specific colonies or plates that are of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: Right now you need to run the entire model successfully once before evaluating this cell. TODO: Add\n",
    "# ability to load in the state of a previous model into a variable called \"inference_model\" or something, and then\n",
    "# pass that model to these functions. As of now that would still require running the first few cells though (NOT the\n",
    "# whole training loop though)\n",
    "### \n",
    "\n",
    "\n",
    "### specific_reconstruction_compare() takes a select set of colonies, and returns a UMAP latent spread of only those colonies,\n",
    "### and a the reconstruction comparison of some of them\n",
    "\n",
    "\n",
    "# Create a master that maps every possible image to the metadata given, but does not find latent vector for each image\n",
    "# (saves on computation and time). Also creates unique \"colony IDs\" for each colony.\n",
    "def create_and_save_full_master(arguments, full_dataset):\n",
    "    file_dir = arguments['METADATA_DIR']\n",
    "    meta_table = pd.read_csv(file_dir, sep='\\t',encoding='ISO-8859-1')\n",
    "\n",
    "    dataset_all_indices = list(range(len(full_dataset)))\n",
    "    filenames = full_dataset.get_image_filename(dataset_all_indices)\n",
    "    df = pd.DataFrame({'filenames': filenames})\n",
    "\n",
    "    # Extract plate number\n",
    "    plate_numbers = df['filenames'].str.extract('Pl(\\d+)|P(\\d+)|Pwt', expand=False)\n",
    "    df['Plate'] = np.where(plate_numbers[0].notnull(), plate_numbers[0], plate_numbers[1])\n",
    "    df['Plate'] = df['Plate'].fillna(-1).astype(int)\n",
    "    # Extract medium\n",
    "    df['medium'] = df['filenames'].str.extract('(spider|ctrl|spdr|control|serum|RPMI|YPD)', expand=False)\n",
    "    df['medium'] = df['medium'].replace({'spdr': 'spider', 'ctrl': 'control'})\n",
    "    # Extract day\n",
    "    df['day'] = df['filenames'].str.extract('day(\\d+)', expand=False).astype(int)\n",
    "    # Extract replicate\n",
    "    df['replicate'] = df['filenames'].str.extract(r'_(\\d+)-', expand=False).fillna('-1')\n",
    "    df['replicate'] = df['replicate'].astype(int)\n",
    "    # Extract position\n",
    "    df['Position'] = df['filenames'].str.extract('-(.*)\\\\.', expand=False)\n",
    "\n",
    "    # Create a function to format position\n",
    "    def get_row_col(pos):\n",
    "        # print(pos)\n",
    "        m = re.search(r\"r(\\d+)-c(\\d+)\", pos)\n",
    "        # print(m)\n",
    "        if m is not None:\n",
    "            row = chr(int(m.group(1)) + 64)  # converting to ASCII\n",
    "            col = m.group(2)\n",
    "            return row + col\n",
    "        else:\n",
    "            return pos\n",
    "\n",
    "    df['Position'] = df['Position'].apply(get_row_col)\n",
    "\n",
    "    full_MASTER = pd.merge(df, meta_table, on=[\"Plate\", \"Position\"], how='inner')\n",
    "\n",
    "    ## Now add columns for the unique ID of each colony, and for the plate name (containing the 96 colonies) for each colony\n",
    "    def generate_unique_id(filename):\n",
    "        hash_out = hashlib.sha256(filename.encode())\n",
    "        unique_id = hash_out.hexdigest()[:8]\n",
    "        return unique_id\n",
    "\n",
    "    def extract_plate_name(full_path):\n",
    "        base_name = os.path.basename(full_path)\n",
    "        name_without_extension = os.path.splitext(base_name)[0]\n",
    "        if name_without_extension[-2] == 'c': # essentially checks if column is 10 or higher, as that changes the length\n",
    "            plate_name = name_without_extension[:-6] # Chop off the last 6 characters\n",
    "        else:\n",
    "            plate_name = name_without_extension[:-7] # Chop off the last 7 characters\n",
    "        return plate_name\n",
    "\n",
    "    full_MASTER['unique_ID'] = full_MASTER['filenames'].apply(generate_unique_id)\n",
    "    full_MASTER['plate_name'] = full_MASTER['filenames'].apply(extract_plate_name)\n",
    "\n",
    "    file_name = \"FULL_MASTER.csv\"\n",
    "    full_MASTER.to_csv(os.path.join(arguments['MASTER_SAVES'], file_name), index=False)\n",
    "\n",
    "# Call the function defined above, and load in the FULL_MASTER\n",
    "create_and_save_full_master(arguments, full_dataset)\n",
    "FULL_MASTER = pd.read_csv(os.path.join(arguments['MASTER_SAVES'], \"FULL_MASTER.csv\")) \n",
    "# Creates a mapping that groups together all the colony IDs for each plate (with 96 colonies)\n",
    "plate_to_colony_mapping = FULL_MASTER.groupby('plate_name')['unique_ID'].apply(list).to_dict() \n",
    "\n",
    "def get_col_ids_from_plates(plate_list):\n",
    "    id_list = []\n",
    "    for plate in plate_list:\n",
    "        ids = plate_to_colony_mapping.get(plate, [])\n",
    "        id_list.extend(ids)\n",
    "    return id_list\n",
    "\n",
    "plates_of_interest = [\"P1_control_day5_1\",\n",
    "                      \"P17_YPD_day2_2\",\n",
    "                      \"Pl10_RPMI_day5_2\"]\n",
    "\n",
    "colonies_to_analyze = get_col_ids_from_plates(plates_of_interest)\n",
    "\n",
    "# Used in analyze_specific_colonies() to reconstruct specific colonies\n",
    "def specific_reconstruction_compare(arguments, reconstruction_tuple, encoder, decoder, ids_to_reconstruct):\n",
    "\n",
    "    if ids_to_reconstruct is None: # If no ids are given, reconstruct 10 random colonies from the plate(s)\n",
    "        plt.figure(figsize=(16,4.5)) \n",
    "        n = 10\n",
    "        random_indices = np.random.choice(len(reconstruction_tuple[0]), size=n, replace=False)\n",
    "        for i, idx in enumerate(random_indices):\n",
    "            ax = plt.subplot(2, n, i+1)\n",
    "            img = reconstruction_tuple[0][idx][0].unsqueeze(0).to(arguments['DEVICE'])\n",
    "            OH_tensor = reconstruction_tuple[0][idx][1]\n",
    "            OH_tensor = OH_tensor.to(arguments['DEVICE'])\n",
    "            # print(OH_tensor)\n",
    "            # print(img.shape)\n",
    "            # OH_tensor = OH_tensor.unsqueeze(-1) # Comment this out omce using variables again\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "                rec_img  = decoder(encoder(img, OH_tensor), OH_tensor) ## ADD INPUT FOR OH TENSOR\n",
    "            plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)  \n",
    "            if i == n//2:\n",
    "                ax.set_title('Original images')\n",
    "            ax = plt.subplot(2, n, i + 1 + n)\n",
    "            plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)  \n",
    "            if i == n//2:\n",
    "                ax.set_title('Reconstructed images')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(16,4.5)) \n",
    "        id_list = reconstruction_tuple[1]\n",
    "        for i, id in enumerate(ids_to_reconstruct):\n",
    "            idx = id_list.index(id)  # find the index of the id in id_list\n",
    "            ax = plt.subplot(2, len(ids_to_reconstruct), i+1)\n",
    "            img = reconstruction_tuple[0][idx][0].unsqueeze(0).to(arguments['DEVICE'])\n",
    "            OH_tensor = reconstruction_tuple[0][idx][1]\n",
    "            OH_tensor = OH_tensor.to(arguments['DEVICE'])\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "                rec_img  = decoder(encoder(img, OH_tensor), OH_tensor)\n",
    "            plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)  \n",
    "            if i == len(ids_to_reconstruct)//2:\n",
    "                ax.set_title('Original images')\n",
    "            ax = plt.subplot(2, len(ids_to_reconstruct), i + 1 + len(ids_to_reconstruct))\n",
    "            plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)  \n",
    "            if i == len(ids_to_reconstruct)//2:\n",
    "                ax.set_title('Reconstructed images')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# This function should take only the colonies of interest for analysis, and create a mini-master with just those colonies\n",
    "# and their latent vectors, and then get the UMAP/t-SNE coords for each, and do whichever visual analyses we want\n",
    "def analyze_specific_colonies(full_master, full_dataset, vae, col_id_list, reduction_technique, anno_type, ids_to_reconstruct, arguments):\n",
    "    \n",
    "    dataset_all_indices = list(range(len(full_dataset)))\n",
    "    all_filenames = full_dataset.get_image_filename(dataset_all_indices)\n",
    "\n",
    "    filtered_master = full_master[full_master['unique_ID'].isin(col_id_list)]\n",
    "    \n",
    "    # get the latent vectors for the colonies of interest\n",
    "    filenames_set = set(filtered_master['filenames']) # for faster lookup\n",
    "    indices_of_interest = [i for i, filename in enumerate(all_filenames) if filename in filenames_set]\n",
    "    df_with_latentvecs = get_latent_vectors(dataset=full_dataset,indices=indices_of_interest,encoder=vae.encoder,arguments=arguments)\n",
    "    filtered_master = filtered_master.merge(df_with_latentvecs, left_on='filenames', right_on='file_name', how='left')\n",
    "    filtered_master.drop(columns=['filenames'], inplace=True) # delete duplicate file name column remnant from merge\n",
    "    \n",
    "    # Create a tuple in the form ((dataset_tuple), unique_id) which can be passed to the reconstructor function\n",
    "    tuples_of_interest = [full_dataset[i] for i in indices_of_interest]\n",
    "    filtered_filenames = [all_filenames[i] for i in indices_of_interest]\n",
    "    filename_to_id = filtered_master.set_index('file_name')['unique_ID'].to_dict() # for fast lookup\n",
    "    unique_ids = [filename_to_id[filename] for filename in filtered_filenames]\n",
    "    reconstruct_tuple = (tuples_of_interest, unique_ids)\n",
    "\n",
    "    # Add columns for the UMAP and t-SNE coordinates:\n",
    "    ### Perform t-SNE dimensionality reduction on latent space ### \n",
    "    features = filtered_master.filter(regex=('^V\\d+'))\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    filtered_master['tsne-2d-one'] = tsne_results[:,0]\n",
    "    filtered_master['tsne-2d-two'] = tsne_results[:,1]\n",
    "    ### Perform UMAP dimensionality reduction on latent space ### \n",
    "    umap = UMAP(n_components=2, random_state=42, min_dist=0.2, n_neighbors=15)\n",
    "    umap_results = umap.fit_transform(features)\n",
    "    filtered_master['umap-2d-one'] = umap_results[:,0]\n",
    "    filtered_master['umap-2d-two'] = umap_results[:,1]\n",
    "\n",
    "    # Generate image scatter and annotated scatter using existing functions\n",
    "    create_img_scatterplot(filtered_master,arguments,reduction_technique)\n",
    "    create_annotated_scatterplot(filtered_master,anno_type)\n",
    "\n",
    "    specific_reconstruction_compare(arguments, reconstruct_tuple, vae.encoder, vae.decoder, ids_to_reconstruct=ids_to_reconstruct) \n",
    "    specific_reconstruction_compare(arguments, reconstruct_tuple, vae.encoder, vae.decoder, ids_to_reconstruct=None) # Do it again with 10 random colonies\n",
    "\n",
    "    return filtered_master\n",
    "\n",
    "analyzed_colonies = analyze_specific_colonies(FULL_MASTER,full_dataset,vae,colonies_to_analyze,'tsne',\"medium\",[\"36f3a141\",\"1c89d1a9\",\"9f449d55\"],arguments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_tut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
